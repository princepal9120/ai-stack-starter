---
title: Guides Overview
description: In-depth guides for configuring AI Stack components
---

## Overview

These guides provide detailed walkthroughs for configuring and optimizing each component of your AI Stack project.

## Getting Started

If you're new to AI Stack, start with the [Quick Start](/docs) to create your first project, then explore these guides based on your needs.

## Available Guides

### LLM Providers

Learn how to configure and switch between LLM providers:

- [LLM Providers](/docs/guides/llm-providers) - OpenAI, Anthropic, Novita, Ollama

### Vector Databases

Set up vector storage for semantic search and RAG:

- [Vector Databases](/docs/guides/vector-databases) - Qdrant, Weaviate, pgvector, Pinecone

### RAG Pipeline

Build production-ready retrieval-augmented generation:

- [RAG Pipeline](/docs/guides/rag-pipeline) - Document ingestion, chunking, retrieval

### Deployment

Deploy your AI Stack application:

- [Deployment](/docs/guides/deployment) - Vercel, Docker, Kubernetes

## Architecture Patterns

Each guide includes:
- **Configuration examples** - Copy-paste ready code
- **Best practices** - Production-tested patterns
- **Troubleshooting** - Common issues and solutions
- **Performance tips** - Optimization strategies

## Need Help?

- [FAQ](/docs/faq) - Common questions
- [GitHub Issues](https://github.com/princepal9120/ai-stack/issues) - Report bugs
- [Contributing](/docs/contributing) - Help improve the docs
