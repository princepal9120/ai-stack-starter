[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.6","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://ai-stack.dev\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","content-config-digest","e877789ab6501b3a","docs",["Map",11,12,21,22,30,31,39,40,48,49,57,58,66,67,75,76,84,85,93,94,102,103,111,112,120,121,129,130,138,139,147,148],"guides",{"id":11,"data":13,"body":16,"filePath":17,"digest":18,"legacyId":19,"deferredRender":20},{"title":14,"description":15},"Guides Overview","In-depth guides for configuring AI Stack components","## Overview\n\nThese guides provide detailed walkthroughs for configuring and optimizing each component of your AI Stack project.\n\n## Getting Started\n\nIf you're new to AI Stack, start with the [Quick Start](/docs) to create your first project, then explore these guides based on your needs.\n\n## Available Guides\n\n### LLM Providers\n\nLearn how to configure and switch between LLM providers:\n\n- [LLM Providers](/docs/guides/llm-providers) - OpenAI, Anthropic, Novita, Ollama\n\n### Vector Databases\n\nSet up vector storage for semantic search and RAG:\n\n- [Vector Databases](/docs/guides/vector-databases) - Qdrant, Weaviate, pgvector, Pinecone\n\n### RAG Pipeline\n\nBuild production-ready retrieval-augmented generation:\n\n- [RAG Pipeline](/docs/guides/rag-pipeline) - Document ingestion, chunking, retrieval\n\n### Deployment\n\nDeploy your AI Stack application:\n\n- [Deployment](/docs/guides/deployment) - Vercel, Docker, Kubernetes\n\n## Architecture Patterns\n\nEach guide includes:\n- **Configuration examples** - Copy-paste ready code\n- **Best practices** - Production-tested patterns\n- **Troubleshooting** - Common issues and solutions\n- **Performance tips** - Optimization strategies\n\n## Need Help?\n\n- [FAQ](/docs/faq) - Common questions\n- [GitHub Issues](https://github.com/princepal9120/ai-stack/issues) - Report bugs\n- [Contributing](/docs/contributing) - Help improve the docs","src/content/docs/guides/index.mdx","12d95855a698cff6","guides/index.mdx",true,"cli/compatibility",{"id":21,"data":23,"body":26,"filePath":27,"digest":28,"legacyId":29,"deferredRender":20},{"title":24,"description":25},"Compatibility","Valid combinations of architecture, database, and provider options","## Overview\n\nNot all options are compatible with each other. This page documents valid combinations to help you choose the right stack.\n\n## Architecture Compatibility\n\n### Next.js Fullstack\n\n| Option | Compatible Values |\n|--------|-------------------|\n| Database | neon, supabase, turso, sqlite |\n| ORM | drizzle, prisma |\n| Auth | better-auth, nextauth, clerk, none |\n| LLM | openai, anthropic, novita, ollama |\n| Vector | qdrant, weaviate, pgvector, pinecone, none |\n\n### FastAPI + Next.js\n\n| Option | Compatible Values |\n|--------|-------------------|\n| Database | neon, supabase, sqlite (via SQLAlchemy) |\n| ORM | sqlalchemy (default, no choice) |\n| Auth | clerk, none (JWT-based) |\n| LLM | openai, anthropic, novita, ollama |\n| Vector | qdrant, weaviate, pgvector, pinecone |\n\n> **Note:** FastAPI uses SQLAlchemy 2.0 with async support.\n\n### TypeScript Backend\n\n| Option | Compatible Values |\n|--------|-------------------|\n| Backend | hono, nestjs, fastify |\n| Database | neon, supabase, turso, sqlite |\n| ORM | drizzle, prisma |\n| Auth | better-auth, clerk, none |\n| LLM | openai, anthropic, novita, ollama |\n| Vector | qdrant, weaviate, pgvector, pinecone, none |\n\n## Database + ORM Compatibility\n\n| Database | Drizzle | Prisma | SQLAlchemy |\n|----------|---------|--------|------------|\n| Neon (Postgres) | ✅ | ✅ | ✅ |\n| Supabase (Postgres) | ✅ | ✅ | ✅ |\n| Turso (SQLite) | ✅ | ⚠️ | ❌ |\n| SQLite | ✅ | ✅ | ✅ |\n\n> ⚠️ Turso + Prisma requires `@prisma/adapter-libsql`\n\n## Vector DB + Database Compatibility\n\n### pgvector\n\npgvector requires PostgreSQL, so it's only compatible with:\n- Neon\n- Supabase\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --database supabase \\\n  --vector pgvector\n```\n\n### Standalone Vector DBs\n\nQdrant, Weaviate, and Pinecone work with any database:\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --database turso \\\n  --vector qdrant\n```\n\n## Auth Compatibility\n\n| Auth | Next.js | FastAPI | TypeScript Backend |\n|------|---------|---------|-------------------|\n| Better Auth | ✅ | ❌ | ✅ |\n| NextAuth | ✅ | ❌ | ❌ |\n| Clerk | ✅ | ✅ | ✅ |\n| None | ✅ | ✅ | ✅ |\n\n## Common Valid Stacks\n\n### Production Next.js\n\n```bash\n--architecture nextjs \\\n--database neon \\\n--auth better-auth \\\n--llm openai \\\n--vector qdrant\n```\n\n### ML/Python Team\n\n```bash\n--architecture fastapi \\\n--database supabase \\\n--auth clerk \\\n--llm anthropic \\\n--vector weaviate\n```\n\n### Edge-First\n\n```bash\n--architecture typescript \\\n--backend hono \\\n--database turso \\\n--auth better-auth \\\n--llm openai\n```","src/content/docs/cli/compatibility.mdx","95b390b0aa30f7d7","cli/compatibility.mdx","index",{"id":30,"data":32,"body":35,"filePath":36,"digest":37,"legacyId":38,"deferredRender":20},{"title":33,"description":34},"Quick Start","Create your first AI Stack project in minutes","## Philosophy\n\n- **Zero vendor lock-in**: Swap LLMs or Vector DBs with one environment variable.\n- **Production-ready**: Built-in auth, error handling, observability, and cost tracking.\n- **Type-safe**: Full TypeScript + Pydantic across the entire stack.\n- **Free and open source**: Forever.\n\n## Get Started\n\n### Prerequisites\n\n- **Node.js LTS** - [Download from nodejs.org](https://nodejs.org/)\n- **Git** - [Download from git-scm.com](https://git-scm.com/)\n- **Docker** (optional) - For running local vector databases\n\n### CLI Installation\n\n```bash\n# npm\nnpx create-ai-stack-starter@latest\n\n# pnpm\npnpm create ai-stack-starter@latest\n\n# bun\nbun create ai-stack-starter@latest\n```\n\nFollow the interactive prompts to choose your architecture, database, auth, LLM provider, and add-ons.\n\n### Skip Prompts (Default Stack)\n\n```bash\nnpx create-ai-stack-starter@latest my-app --yes\n```\n\n### Stack Builder (UI)\n\nVisit [/builder](/builder) to pick your stack visually and copy the generated command.\n\n## Common Setups\n\n### Next.js Fullstack (Recommended)\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --architecture nextjs \\\n  --database neon \\\n  --auth better-auth \\\n  --llm openai \\\n  --addons tailwind,biome\n```\n\n### FastAPI + Next.js (ML Teams)\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --architecture fastapi \\\n  --database supabase \\\n  --auth clerk \\\n  --llm anthropic \\\n  --vector qdrant\n```\n\n### TypeScript Backend (Edge)\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --architecture typescript \\\n  --backend hono \\\n  --database turso \\\n  --auth better-auth \\\n  --llm openai\n```\n\n## Flags Cheat Sheet\n\nSee the full list in the [CLI Reference](/docs/cli). Key flags:\n\n- `--architecture`: nextjs, fastapi, typescript\n- `--database`: neon, supabase, turso, sqlite\n- `--auth`: better-auth, nextauth, clerk, none\n- `--llm`: openai, anthropic, novita, ollama\n- `--vector`: qdrant, weaviate, pgvector, pinecone\n- `--addons`: tailwind, biome, pwa, analytics\n\n## Next Steps\n\n- [CLI Reference](/docs/cli) - Flags, usage, and examples\n- [Project Structure](/docs/project-structure) - See how projects are generated\n- [Guides](/docs/guides) - LLM providers, vector databases, RAG pipelines\n- [FAQ](/docs/faq) - Common questions answered","src/content/docs/index.mdx","cfc49c0669c42d1f","index.mdx","cli/prompts",{"id":39,"data":41,"body":44,"filePath":45,"digest":46,"legacyId":47,"deferredRender":20},{"title":42,"description":43},"CLI Prompts","Details on interactive prompts during project creation","## Overview\n\nWhen you run `npx create-ai-stack-starter@latest` without flags, you'll be guided through interactive prompts to configure your project.\n\n## Prompt Flow\n\n### 1. Project Name\n\n```\n? What is your project name? › my-ai-app\n```\n\nEnter the name for your project directory. Must be a valid npm package name.\n\n### 2. Architecture Selection\n\n```\n? Select your architecture:\n❯ Next.js Fullstack - Rapid prototyping, Vercel deployment\n  FastAPI + Next.js - ML teams, complex AI pipelines\n  TypeScript Backend - TypeScript teams, edge deployment\n```\n\nChoose based on your team's expertise and deployment target.\n\n### 3. Database Provider\n\n```\n? Select your database:\n❯ Neon - Serverless Postgres, auto-scaling\n  Supabase - Postgres with realtime, auth, storage\n  Turso - Edge SQLite, globally distributed\n  SQLite - Local development only\n```\n\n### 4. Authentication\n\n```\n? Select authentication provider:\n❯ Better Auth - Modern, type-safe, self-hosted\n  NextAuth.js - Flexible OAuth, social logins\n  Clerk - Managed auth, great DX\n  None - No authentication\n```\n\n### 5. LLM Provider\n\n```\n? Select your LLM provider:\n❯ OpenAI - GPT-4o, GPT-4, GPT-3.5\n  Anthropic - Claude 3.5, Claude 3\n  Novita AI - Cost-effective inference\n  Ollama - Local models, privacy-first\n```\n\n### 6. Vector Database\n\n```\n? Select your vector database:\n❯ Qdrant - High-performance, open-source\n  Weaviate - AI-native, GraphQL\n  pgvector - PostgreSQL extension\n  Pinecone - Managed, serverless\n  None - No vector search\n```\n\n### 7. Add-ons\n\n```\n? Select add-ons (space to toggle):\n◉ Tailwind CSS\n◉ Biome (linter/formatter)\n◯ PWA support\n◯ Analytics\n◯ Docker Compose\n```\n\nUse space to toggle selections, enter to confirm.\n\n## Skipping Prompts\n\n### Skip All with `--yes`\n\n```bash\nnpx create-ai-stack-starter@latest my-app --yes\n```\n\nUses sensible defaults:\n- Architecture: `nextjs`\n- Database: `neon`\n- Auth: `better-auth`\n- LLM: `openai`\n- Add-ons: `tailwind`, `biome`\n\n### Skip Specific with Flags\n\nAny flag you provide skips that prompt:\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --architecture fastapi \\\n  --llm anthropic\n# Only prompts for database, auth, vector, add-ons\n```","src/content/docs/cli/prompts.mdx","ad8338931d685180","cli/prompts.mdx","cli",{"id":48,"data":50,"body":53,"filePath":54,"digest":55,"legacyId":56,"deferredRender":20},{"title":51,"description":52},"CLI Reference","Complete reference for the AI Stack CLI commands and options","## Overview\n\nThe AI Stack CLI scaffolds production-ready AI applications with your choice of architecture, database, authentication, and AI providers.\n\n## Installation\n\nThe CLI runs directly via your package manager—no global install required:\n\n```bash\n# npm\nnpx create-ai-stack-starter@latest\n\n# pnpm  \npnpm create ai-stack-starter@latest\n\n# bun\nbun create ai-stack-starter@latest\n```\n\n## Basic Usage\n\n### Interactive Mode\n\n```bash\nnpx create-ai-stack-starter@latest my-app\n```\n\nThis launches the interactive prompt where you select:\n1. **Architecture** - Next.js Fullstack, FastAPI + Next.js, or TypeScript Backend\n2. **Database** - Neon, Supabase, Turso, or SQLite\n3. **Authentication** - Better Auth, NextAuth, Clerk, or None\n4. **LLM Provider** - OpenAI, Anthropic, Novita AI, or Ollama\n5. **Vector Database** - Qdrant, Weaviate, pgvector, or Pinecone\n6. **Add-ons** - Tailwind, Biome, PWA, Analytics\n\n### Non-Interactive Mode\n\nSkip prompts with flags:\n\n```bash\nnpx create-ai-stack-starter@latest my-app \\\n  --architecture nextjs \\\n  --database neon \\\n  --auth better-auth \\\n  --llm openai \\\n  --yes\n```\n\n## Commands\n\n### `create` (default)\n\nCreates a new AI Stack project.\n\n```bash\nnpx create-ai-stack-starter@latest [project-name] [options]\n```\n\n**Arguments:**\n- `project-name` - Name of the project directory (default: `my-ai-app`)\n\n**Options:**\n- See [Options](/docs/cli/options) for complete list\n\n### `builder`\n\nOpens the Stack Builder UI in your browser:\n\n```bash\nnpx create-ai-stack-starter@latest builder\n```\n\n## Environment\n\nThe CLI respects these environment variables:\n\n| Variable | Description |\n|----------|-------------|\n| `AIS_TELEMETRY_DISABLED` | Set to `1` to disable anonymous telemetry |\n| `NO_COLOR` | Disable colored output |\n| `DEBUG` | Enable verbose logging |\n\n## Next Steps\n\n- [Options](/docs/cli/options) - All available flags\n- [Prompts](/docs/cli/prompts) - Interactive prompt details\n- [Compatibility](/docs/cli/compatibility) - Valid option combinations\n- [Programmatic API](/docs/cli/programmatic-api) - Use as a library","src/content/docs/cli/index.mdx","22fe64ca4c923e8c","cli/index.mdx","guides/llm-providers",{"id":57,"data":59,"body":62,"filePath":63,"digest":64,"legacyId":65,"deferredRender":20},{"title":60,"description":61},"LLM Providers","Configure and switch between LLM providers in AI Stack","## Overview\n\nAI Stack provides a unified interface for multiple LLM providers. Switch providers with a single environment variable—no code changes required.\n\n## Supported Providers\n\n| Provider | Models | Best For |\n|----------|--------|----------|\n| OpenAI | GPT-4o, GPT-4, GPT-3.5 | General purpose, proven reliability |\n| Anthropic | Claude 3.5, Claude 3 | Long context, safety |\n| Novita AI | Various open-source | Cost optimization |\n| Ollama | Llama, Mistral, etc. | Privacy, local development |\n\n## Configuration\n\n### Environment Variables\n\n```env\n# Provider selection\nLLM_PROVIDER=openai\n\n# Provider API keys\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nNOVITA_API_KEY=...\n\n# For Ollama (local)\nOLLAMA_BASE_URL=http://localhost:11434\n```\n\n### OpenAI\n\n```typescript\n// lib/ai/config.ts\nexport const llmConfig = {\n  provider: 'openai',\n  model: 'gpt-4o',\n  temperature: 0.7,\n  maxTokens: 4096,\n};\n```\n\n**Available Models:**\n- `gpt-4o` - Latest, multimodal\n- `gpt-4o-mini` - Fast, cost-effective\n- `gpt-4-turbo` - High capability\n- `gpt-3.5-turbo` - Budget option\n\n### Anthropic\n\n```typescript\nexport const llmConfig = {\n  provider: 'anthropic',\n  model: 'claude-3-5-sonnet-20241022',\n  temperature: 0.7,\n  maxTokens: 8192,\n};\n```\n\n**Available Models:**\n- `claude-3-5-sonnet-20241022` - Best balance\n- `claude-3-opus-20240229` - Highest capability\n- `claude-3-haiku-20240307` - Fastest\n\n### Novita AI\n\n```typescript\nexport const llmConfig = {\n  provider: 'novita',\n  model: 'meta-llama/llama-3.1-70b-instruct',\n  temperature: 0.7,\n};\n```\n\n### Ollama (Local)\n\n```typescript\nexport const llmConfig = {\n  provider: 'ollama',\n  model: 'llama3.1',\n  baseUrl: 'http://localhost:11434',\n};\n```\n\n## Switching Providers\n\nChange providers without code modifications:\n\n```bash\n# Development with Ollama\nLLM_PROVIDER=ollama npm run dev\n\n# Production with OpenAI\nLLM_PROVIDER=openai npm run start\n```\n\n## Usage\n\n### Basic Completion\n\n```typescript\nimport { createCompletion } from '@/lib/ai';\n\nconst response = await createCompletion({\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Hello!' },\n  ],\n});\n```\n\n### Streaming\n\n```typescript\nimport { streamCompletion } from '@/lib/ai';\n\nconst stream = await streamCompletion({\n  messages: [...],\n});\n\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.content);\n}\n```\n\n## Best Practices\n\n1. **Use environment variables** - Never hardcode API keys\n2. **Implement fallbacks** - Switch providers on rate limits\n3. **Monitor costs** - Track token usage per provider\n4. **Test locally** - Use Ollama for development\n\n## Troubleshooting\n\n### API Key Issues\n\n```bash\n# Verify key is set\necho $OPENAI_API_KEY\n\n# Check in .env.local\ncat .env.local | grep API_KEY\n```\n\n### Rate Limits\n\nImplement exponential backoff:\n\n```typescript\nimport { createCompletion } from '@/lib/ai';\n\nasync function withRetry(fn, maxRetries = 3) {\n  for (let i = 0; i \u003C maxRetries; i++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (error.status === 429) {\n        await new Promise(r => setTimeout(r, 2 ** i * 1000));\n        continue;\n      }\n      throw error;\n    }\n  }\n}\n```","src/content/docs/guides/llm-providers.mdx","6d71b250a60d3520","guides/llm-providers.mdx","cli/programmatic-api",{"id":66,"data":68,"body":71,"filePath":72,"digest":73,"legacyId":74,"deferredRender":20},{"title":69,"description":70},"Programmatic API","Use AI Stack CLI as a library in your Node.js projects","## Overview\n\nYou can use the AI Stack CLI programmatically in your Node.js or build scripts.\n\n## Installation\n\n```bash\nnpm install create-ai-stack-starter\n```\n\n## Basic Usage\n\n```typescript\nimport { createProject } from 'create-ai-stack-starter';\n\nawait createProject({\n  name: 'my-ai-app',\n  architecture: 'nextjs',\n  database: 'neon',\n  auth: 'better-auth',\n  llm: 'openai',\n  vector: 'qdrant',\n  addons: ['tailwind', 'biome'],\n});\n```\n\n## Options Interface\n\n```typescript\ninterface CreateProjectOptions {\n  // Required\n  name: string;\n  \n  // Architecture\n  architecture: 'nextjs' | 'fastapi' | 'typescript';\n  backend?: 'hono' | 'nestjs' | 'fastify';\n  \n  // Database\n  database: 'neon' | 'supabase' | 'turso' | 'sqlite';\n  orm?: 'drizzle' | 'prisma';\n  \n  // Auth\n  auth: 'better-auth' | 'nextauth' | 'clerk' | 'none';\n  \n  // AI\n  llm: 'openai' | 'anthropic' | 'novita' | 'ollama';\n  vector?: 'qdrant' | 'weaviate' | 'pgvector' | 'pinecone' | 'none';\n  \n  // Add-ons\n  addons?: Array\u003C'tailwind' | 'biome' | 'pwa' | 'analytics' | 'docker'>;\n  \n  // Behavior\n  git?: boolean;      // Initialize git repo (default: true)\n  install?: boolean;  // Install dependencies (default: true)\n  cwd?: string;       // Working directory\n}\n```\n\n## Advanced Usage\n\n### Custom Output Directory\n\n```typescript\nawait createProject({\n  name: 'my-app',\n  architecture: 'nextjs',\n  database: 'neon',\n  auth: 'better-auth',\n  llm: 'openai',\n  cwd: '/custom/path',\n});\n```\n\n### Skip Git and Install\n\n```typescript\nawait createProject({\n  name: 'my-app',\n  architecture: 'nextjs',\n  database: 'neon',\n  auth: 'better-auth',\n  llm: 'openai',\n  git: false,\n  install: false,\n});\n```\n\n### With Callback\n\n```typescript\nimport { createProject, type Progress } from 'create-ai-stack-starter';\n\nawait createProject(\n  {\n    name: 'my-app',\n    architecture: 'nextjs',\n    database: 'neon',\n    auth: 'better-auth',\n    llm: 'openai',\n  },\n  {\n    onProgress: (progress: Progress) => {\n      console.log(`${progress.step}: ${progress.message}`);\n    },\n  }\n);\n```\n\n## Return Value\n\n```typescript\ninterface CreateResult {\n  projectPath: string;\n  config: ResolvedConfig;\n}\n\nconst result = await createProject({ ... });\nconsole.log(`Created at: ${result.projectPath}`);\n```\n\n## Error Handling\n\n```typescript\nimport { createProject, IncompatibleOptionsError } from 'create-ai-stack-starter';\n\ntry {\n  await createProject({\n    name: 'my-app',\n    architecture: 'fastapi',\n    auth: 'better-auth', // Not compatible with FastAPI\n    // ...\n  });\n} catch (error) {\n  if (error instanceof IncompatibleOptionsError) {\n    console.error('Invalid combination:', error.message);\n  }\n}\n```","src/content/docs/cli/programmatic-api.mdx","520a655a9383dae6","cli/programmatic-api.mdx","cli/options",{"id":75,"data":77,"body":80,"filePath":81,"digest":82,"legacyId":83,"deferredRender":20},{"title":78,"description":79},"CLI Options","Complete reference for all AI Stack CLI flags and options","## Project Options\n\n### `--architecture`\n\nSelect the application architecture:\n\n| Value | Description |\n|-------|-------------|\n| `nextjs` | Next.js 15 fullstack with App Router |\n| `fastapi` | FastAPI backend + Next.js frontend |\n| `typescript` | TypeScript backend (Hono/NestJS/Fastify) |\n\n```bash\nnpx create-ai-stack-starter@latest --architecture fastapi\n```\n\n### `--backend`\n\nSelect the backend framework (for TypeScript architecture):\n\n| Value | Description |\n|-------|-------------|\n| `hono` | Lightweight, edge-ready |\n| `nestjs` | Enterprise-grade, modular |\n| `fastify` | High-performance |\n\n### `--database`\n\nSelect the database provider:\n\n| Value | Description |\n|-------|-------------|\n| `neon` | Serverless Postgres |\n| `supabase` | Postgres with realtime |\n| `turso` | Edge SQLite (libSQL) |\n| `sqlite` | Local SQLite |\n\n### `--orm`\n\nSelect the ORM:\n\n| Value | Description |\n|-------|-------------|\n| `drizzle` | TypeScript-first, lightweight |\n| `prisma` | Full-featured, great DX |\n\n## Authentication Options\n\n### `--auth`\n\nSelect the authentication provider:\n\n| Value | Description |\n|-------|-------------|\n| `better-auth` | Modern, type-safe, self-hosted |\n| `nextauth` | Flexible OAuth, social logins |\n| `clerk` | Managed auth with great DX |\n| `none` | No authentication |\n\n```bash\nnpx create-ai-stack-starter@latest --auth clerk\n```\n\n## AI Options\n\n### `--llm`\n\nSelect the LLM provider:\n\n| Value | Description |\n|-------|-------------|\n| `openai` | GPT-4o, GPT-4, GPT-3.5 |\n| `anthropic` | Claude 3.5, Claude 3 |\n| `novita` | Cost-effective inference |\n| `ollama` | Local models |\n\n### `--vector`\n\nSelect the vector database:\n\n| Value | Description |\n|-------|-------------|\n| `qdrant` | High-performance, open-source |\n| `weaviate` | AI-native, GraphQL API |\n| `pgvector` | PostgreSQL extension |\n| `pinecone` | Managed, serverless |\n\n```bash\nnpx create-ai-stack-starter@latest --llm anthropic --vector qdrant\n```\n\n## Add-ons\n\n### `--addons`\n\nComma-separated list of add-ons:\n\n| Value | Description |\n|-------|-------------|\n| `tailwind` | Tailwind CSS styling |\n| `biome` | Fast linter and formatter |\n| `pwa` | Progressive Web App support |\n| `analytics` | Usage analytics |\n| `docker` | Docker Compose setup |\n\n```bash\nnpx create-ai-stack-starter@latest --addons tailwind,biome,docker\n```\n\n## Behavior Flags\n\n### `--yes` / `-y`\n\nAccept all defaults without prompting:\n\n```bash\nnpx create-ai-stack-starter@latest my-app --yes\n```\n\n### `--git`\n\nInitialize a git repository (default: true):\n\n```bash\nnpx create-ai-stack-starter@latest --no-git\n```\n\n### `--install`\n\nInstall dependencies (default: true):\n\n```bash\nnpx create-ai-stack-starter@latest --no-install\n```","src/content/docs/cli/options.mdx","ed3c9460481ee883","cli/options.mdx","guides/vector-databases",{"id":84,"data":86,"body":89,"filePath":90,"digest":91,"legacyId":92,"deferredRender":20},{"title":87,"description":88},"Vector Databases","Configure vector storage for semantic search and RAG","## Overview\n\nVector databases enable semantic search by storing and querying high-dimensional embeddings. AI Stack provides a unified interface for multiple vector database providers.\n\n## Supported Providers\n\n| Provider | Type | Best For |\n|----------|------|----------|\n| Qdrant | Standalone | High performance, open-source |\n| Weaviate | Standalone | AI-native, GraphQL API |\n| pgvector | Extension | Existing Postgres users |\n| Pinecone | Managed | Serverless, no ops |\n\n## Configuration\n\n### Environment Variables\n\n```env\n# Provider selection\nVECTOR_DB=qdrant\n\n# Qdrant\nQDRANT_URL=http://localhost:6333\nQDRANT_API_KEY=...\n\n# Weaviate\nWEAVIATE_URL=http://localhost:8080\nWEAVIATE_API_KEY=...\n\n# Pinecone\nPINECONE_API_KEY=...\nPINECONE_INDEX=ai-stack\n```\n\n## Qdrant\n\n### Local Setup (Docker)\n\n```bash\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\n### Configuration\n\n```typescript\n// lib/search/config.ts\nexport const vectorConfig = {\n  provider: 'qdrant',\n  url: process.env.QDRANT_URL,\n  collectionName: 'documents',\n  vectorSize: 1536, // OpenAI embeddings\n};\n```\n\n### Usage\n\n```typescript\nimport { vectorStore } from '@/lib/search';\n\n// Index documents\nawait vectorStore.upsert([\n  { id: '1', vector: embeddings, metadata: { title: 'Doc 1' } },\n]);\n\n// Search\nconst results = await vectorStore.search({\n  vector: queryEmbedding,\n  topK: 5,\n});\n```\n\n## Weaviate\n\n### Local Setup (Docker)\n\n```bash\ndocker run -p 8080:8080 semitechnologies/weaviate\n```\n\n### Configuration\n\n```typescript\nexport const vectorConfig = {\n  provider: 'weaviate',\n  url: process.env.WEAVIATE_URL,\n  className: 'Document',\n};\n```\n\n## pgvector\n\n### Setup\n\nEnable the extension in your Postgres database:\n\n```sql\nCREATE EXTENSION vector;\n\nCREATE TABLE documents (\n  id SERIAL PRIMARY KEY,\n  content TEXT,\n  embedding vector(1536)\n);\n\nCREATE INDEX ON documents \n  USING ivfflat (embedding vector_cosine_ops);\n```\n\n### Configuration\n\n```typescript\nexport const vectorConfig = {\n  provider: 'pgvector',\n  connectionString: process.env.DATABASE_URL,\n  tableName: 'documents',\n};\n```\n\n## Pinecone\n\n### Configuration\n\n```typescript\nexport const vectorConfig = {\n  provider: 'pinecone',\n  apiKey: process.env.PINECONE_API_KEY,\n  index: 'ai-stack',\n  namespace: 'production',\n};\n```\n\n## Unified Interface\n\nAI Stack provides a consistent API regardless of provider:\n\n```typescript\nimport { createVectorStore } from '@/lib/search';\n\nconst store = createVectorStore();\n\n// Works with any configured provider\nawait store.upsert(documents);\nconst results = await store.search(query, { topK: 10 });\nawait store.delete(ids);\n```\n\n## Embedding Models\n\n### OpenAI Embeddings\n\n```typescript\nimport { createEmbedding } from '@/lib/ai';\n\nconst embedding = await createEmbedding('Hello world');\n// Returns: number[1536]\n```\n\n### Local Embeddings (Ollama)\n\n```typescript\nexport const embeddingConfig = {\n  provider: 'ollama',\n  model: 'nomic-embed-text',\n};\n```\n\n## Best Practices\n\n1. **Choose the right distance metric** - Cosine for normalized, Euclidean for absolute\n2. **Batch operations** - Upsert in batches of 100-1000\n3. **Index appropriately** - Use IVF for large datasets\n4. **Monitor performance** - Track query latency\n\n## Troubleshooting\n\n### Connection Issues\n\n```bash\n# Test Qdrant connection\ncurl http://localhost:6333/health\n\n# Test Weaviate connection\ncurl http://localhost:8080/v1/.well-known/ready\n```\n\n### Dimension Mismatch\n\nEnsure embedding dimensions match vector index:\n- OpenAI `text-embedding-3-small`: 1536\n- OpenAI `text-embedding-3-large`: 3072\n- Ollama `nomic-embed-text`: 768","src/content/docs/guides/vector-databases.mdx","c939ae1ba5ebad20","guides/vector-databases.mdx","guides/rag-pipeline",{"id":93,"data":95,"body":98,"filePath":99,"digest":100,"legacyId":101,"deferredRender":20},{"title":96,"description":97},"RAG Pipeline","Build production-ready retrieval-augmented generation","## Overview\n\nRetrieval-Augmented Generation (RAG) enhances LLM responses with relevant context from your documents. AI Stack provides a complete pipeline for document ingestion, chunking, embedding, and retrieval.\n\n## Architecture\n\n```\nDocuments → Chunking → Embedding → Vector Store\n                                        ↓\nQuery → Embedding → Search → Context → LLM → Response\n```\n\n## Quick Start\n\n```typescript\nimport { rag } from '@/lib/rag';\n\n// Ingest documents\nawait rag.ingest([\n  { content: 'Your document text...' },\n]);\n\n// Query with retrieval\nconst response = await rag.query('What is the refund policy?');\n```\n\n## Document Ingestion\n\n### Supported Formats\n\n| Format | Extension | Library |\n|--------|-----------|---------|\n| PDF | `.pdf` | pdf-parse |\n| Markdown | `.md` | marked |\n| Text | `.txt` | native |\n| HTML | `.html` | cheerio |\n| Word | `.docx` | mammoth |\n\n### Ingestion API\n\n```typescript\nimport { ingestDocuments } from '@/lib/rag';\n\nawait ingestDocuments({\n  source: './docs',\n  patterns: ['**/*.md', '**/*.pdf'],\n  metadata: {\n    source: 'documentation',\n    version: '1.0',\n  },\n});\n```\n\n## Chunking Strategies\n\n### Fixed Size\n\n```typescript\nconst chunks = await chunk(document, {\n  strategy: 'fixed',\n  chunkSize: 512,\n  overlap: 50,\n});\n```\n\n### Semantic\n\n```typescript\nconst chunks = await chunk(document, {\n  strategy: 'semantic',\n  maxChunkSize: 1000,\n  minChunkSize: 100,\n});\n```\n\n### By Heading\n\n```typescript\nconst chunks = await chunk(document, {\n  strategy: 'heading',\n  levels: ['h1', 'h2'],\n});\n```\n\n## Retrieval\n\n### Basic Search\n\n```typescript\nconst results = await rag.search(query, {\n  topK: 5,\n});\n```\n\n### With Filters\n\n```typescript\nconst results = await rag.search(query, {\n  topK: 5,\n  filter: {\n    source: 'documentation',\n    version: { $gte: '1.0' },\n  },\n});\n```\n\n### Hybrid Search\n\nCombine semantic and keyword search:\n\n```typescript\nconst results = await rag.search(query, {\n  mode: 'hybrid',\n  alpha: 0.7, // 70% semantic, 30% keyword\n  topK: 10,\n});\n```\n\n## Reranking\n\nImprove relevance with cross-encoder reranking:\n\n```typescript\nconst results = await rag.search(query, {\n  topK: 20,        // Retrieve more\n  rerank: true,\n  rerankTopK: 5,   // Return top 5 after reranking\n});\n```\n\n## Query Pipeline\n\n### Basic RAG\n\n```typescript\nimport { rag } from '@/lib/rag';\n\nconst response = await rag.query(question, {\n  topK: 5,\n  systemPrompt: 'Answer based only on the provided context.',\n});\n```\n\n### With Citations\n\n```typescript\nconst response = await rag.query(question, {\n  topK: 5,\n  includeCitations: true,\n});\n\n// response.answer: \"The refund policy states... [1]\"\n// response.citations: [{ id: '1', source: 'policy.md', chunk: '...' }]\n```\n\n### Streaming\n\n```typescript\nconst stream = await rag.queryStream(question);\n\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.content);\n}\n```\n\n## Configuration\n\n```typescript\n// lib/rag/config.ts\nexport const ragConfig = {\n  // Chunking\n  chunkSize: 512,\n  chunkOverlap: 50,\n  \n  // Retrieval\n  topK: 5,\n  scoreThreshold: 0.7,\n  \n  // Reranking\n  rerank: true,\n  rerankModel: 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n  \n  // Generation\n  maxTokens: 1024,\n  temperature: 0.3,\n};\n```\n\n## Best Practices\n\n1. **Chunk size matters** - 256-512 tokens for most use cases\n2. **Use overlap** - 10-20% overlap prevents context loss\n3. **Filter aggressively** - Narrow search space with metadata\n4. **Rerank for quality** - Worth the latency for important queries\n5. **Monitor retrieval** - Log and review what's being retrieved\n\n## Evaluation\n\n### Metrics\n\n```typescript\nimport { evaluate } from '@/lib/rag/eval';\n\nconst metrics = await evaluate({\n  queries: testQueries,\n  groundTruth: expectedAnswers,\n});\n\n// metrics.precision, metrics.recall, metrics.mrr\n```\n\n### A/B Testing\n\n```typescript\nconst resultA = await rag.query(q, { topK: 5 });\nconst resultB = await rag.query(q, { topK: 10, rerank: true });\n\n// Log and compare\n```\n\n## Troubleshooting\n\n### Poor Retrieval Quality\n\n1. Check chunk boundaries aren't cutting context\n2. Try different embedding models\n3. Increase topK and use reranking\n4. Add metadata filters\n\n### Slow Queries\n\n1. Add vector index (IVF, HNSW)\n2. Pre-filter with metadata\n3. Reduce topK for initial retrieval\n4. Cache common queries","src/content/docs/guides/rag-pipeline.mdx","aa208449c5bf3a21","guides/rag-pipeline.mdx","guides/deployment",{"id":102,"data":104,"body":107,"filePath":108,"digest":109,"legacyId":110,"deferredRender":20},{"title":105,"description":106},"Deployment","Deploy your AI Stack application to production","## Overview\n\nAI Stack applications can be deployed to various platforms. This guide covers the most common deployment targets.\n\n## Deployment Options\n\n| Platform | Best For | Architecture Support |\n|----------|----------|---------------------|\n| Vercel | Next.js apps | nextjs |\n| Docker | Self-hosted | All |\n| Kubernetes | Enterprise scale | All |\n| Railway | Quick deploys | All |\n\n## Vercel Deployment\n\n### Prerequisites\n\n- Vercel account\n- GitHub repository\n\n### Steps\n\n1. **Connect Repository**\n\n```bash\nnpx vercel link\n```\n\n2. **Set Environment Variables**\n\n```bash\nvercel env add OPENAI_API_KEY\nvercel env add DATABASE_URL\nvercel env add QDRANT_URL\n```\n\n3. **Deploy**\n\n```bash\nvercel --prod\n```\n\n### Configuration\n\n```json\n// vercel.json\n{\n  \"framework\": \"nextjs\",\n  \"buildCommand\": \"npm run build\",\n  \"installCommand\": \"npm install\"\n}\n```\n\n## Docker Deployment\n\n### Dockerfile (Next.js)\n\n```dockerfile\nFROM node:20-alpine AS base\n\nFROM base AS deps\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\n\nFROM base AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\n\nFROM base AS runner\nWORKDIR /app\nENV NODE_ENV=production\n\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nextjs\n\nCOPY --from=builder /app/public ./public\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static\n\nUSER nextjs\nEXPOSE 3000\nENV PORT=3000\n\nCMD [\"node\", \"server.js\"]\n```\n\n### Docker Compose\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  app:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=${DATABASE_URL}\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - QDRANT_URL=http://qdrant:6333\n    depends_on:\n      - qdrant\n\n  qdrant:\n    image: qdrant/qdrant\n    ports:\n      - \"6333:6333\"\n    volumes:\n      - qdrant_data:/qdrant/storage\n\nvolumes:\n  qdrant_data:\n```\n\n### Build and Run\n\n```bash\ndocker-compose up -d\n```\n\n## Kubernetes Deployment\n\n### Deployment Manifest\n\n```yaml\n# k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ai-stack-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ai-stack\n  template:\n    metadata:\n      labels:\n        app: ai-stack\n    spec:\n      containers:\n        - name: app\n          image: your-registry/ai-stack:latest\n          ports:\n            - containerPort: 3000\n          env:\n            - name: DATABASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: ai-stack-secrets\n                  key: database-url\n          resources:\n            requests:\n              memory: \"256Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"512Mi\"\n              cpu: \"500m\"\n```\n\n### Service\n\n```yaml\n# k8s/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: ai-stack-service\nspec:\n  selector:\n    app: ai-stack\n  ports:\n    - port: 80\n      targetPort: 3000\n  type: LoadBalancer\n```\n\n### Deploy\n\n```bash\nkubectl apply -f k8s/\n```\n\n## Railway Deployment\n\n### Quick Deploy\n\n1. Connect your GitHub repository\n2. Railway auto-detects the framework\n3. Add environment variables in dashboard\n4. Deploy\n\n### Railway Config\n\n```toml\n# railway.toml\n[build]\nbuilder = \"nixpacks\"\n\n[deploy]\nstartCommand = \"npm start\"\nhealthcheckPath = \"/api/health\"\n```\n\n## Environment Variables\n\n### Required\n\n| Variable | Description |\n|----------|-------------|\n| `DATABASE_URL` | Database connection string |\n| `OPENAI_API_KEY` | LLM provider API key |\n\n### Optional\n\n| Variable | Description |\n|----------|-------------|\n| `QDRANT_URL` | Vector database URL |\n| `NEXTAUTH_SECRET` | Auth secret (32+ chars) |\n| `NEXTAUTH_URL` | Production URL |\n\n### Secrets Management\n\nFor production, use:\n- Vercel: Environment Variables UI\n- Docker: Docker secrets or `.env` files\n- Kubernetes: Secrets or external secret managers\n\n## Health Checks\n\nAdd a health endpoint:\n\n```typescript\n// app/api/health/route.ts\nexport async function GET() {\n  return Response.json({ status: 'ok', timestamp: Date.now() });\n}\n```\n\n## Monitoring\n\n### Recommended Tools\n\n- **Vercel Analytics** - Built-in for Vercel\n- **Sentry** - Error tracking\n- **PostHog** - Product analytics\n- **Grafana** - Metrics and dashboards\n\n## Checklist\n\n- [ ] Set all required environment variables\n- [ ] Configure database connection\n- [ ] Set up vector database (if using)\n- [ ] Enable HTTPS\n- [ ] Configure CORS\n- [ ] Set up monitoring and alerting\n- [ ] Test health endpoints\n- [ ] Configure auto-scaling (if applicable)","src/content/docs/guides/deployment.mdx","a51b05317520aa80","guides/deployment.mdx","project-structure",{"id":111,"data":113,"body":116,"filePath":117,"digest":118,"legacyId":119,"deferredRender":20},{"title":114,"description":115},"Project Structure","Understanding the structure of projects created by AI Stack CLI","## Overview\n\nAI Stack CLI scaffolds a production-ready project with organized directories for frontend, backend, and AI components. The exact structure depends on your architecture choice.\n\n## Root Layout\n\nAt the repository root you will see:\n\n```\nmy-ai-app/\n├── app/                    # Next.js App Router\n├── components/             # React components\n├── lib/                    # Shared utilities\n├── types/                  # TypeScript types\n├── public/                 # Static assets\n├── .env.example           # Environment template\n├── package.json\n├── tsconfig.json\n├── next.config.ts\n└── README.md\n```\n\n## Architecture-Specific Structures\n\n### Next.js Fullstack\n\n```\nmy-ai-app/\n├── app/\n│   ├── (auth)/            # Auth pages (if auth enabled)\n│   │   ├── login/\n│   │   └── register/\n│   ├── (dashboard)/       # Protected pages\n│   │   └── page.tsx\n│   ├── api/               # API routes\n│   │   ├── chat/\n│   │   ├── auth/\n│   │   └── health/\n│   ├── layout.tsx\n│   └── page.tsx\n├── components/\n│   ├── chat/              # Chat UI components\n│   │   ├── chat-input.tsx\n│   │   ├── chat-message.tsx\n│   │   └── chat-list.tsx\n│   └── ui/                # shadcn/ui components\n├── lib/\n│   ├── ai/                # LLM client abstraction\n│   │   ├── index.ts\n│   │   ├── providers/\n│   │   └── config.ts\n│   ├── auth/              # Auth configuration\n│   ├── db/                # Database schema & client\n│   └── search/            # Vector search client\n└── types/\n```\n\n### FastAPI + Next.js\n\n```\nmy-ai-app/\n├── frontend/              # Next.js application\n│   ├── app/\n│   ├── components/\n│   └── lib/\n├── backend/               # FastAPI application\n│   ├── app/\n│   │   ├── api/\n│   │   │   ├── routes/\n│   │   │   └── deps.py\n│   │   ├── core/\n│   │   │   ├── config.py\n│   │   │   └── security.py\n│   │   ├── models/\n│   │   ├── schemas/\n│   │   └── services/\n│   │       ├── ai.py\n│   │       └── rag.py\n│   ├── alembic/           # Database migrations\n│   ├── requirements.txt\n│   └── main.py\n└── docker-compose.yml\n```\n\n### TypeScript Backend\n\n```\nmy-ai-app/\n├── apps/\n│   ├── web/               # Frontend\n│   │   ├── app/\n│   │   └── components/\n│   └── server/            # Backend\n│       ├── src/\n│       │   ├── routes/\n│       │   ├── services/\n│       │   ├── db/\n│       │   └── index.ts\n│       └── package.json\n├── packages/\n│   └── shared/            # Shared types/utils\n├── turbo.json\n└── package.json\n```\n\n## Key Directories\n\n### `lib/ai/`\n\nLLM provider abstraction:\n\n```\nlib/ai/\n├── index.ts               # Main exports\n├── config.ts              # Provider configuration\n├── providers/\n│   ├── openai.ts\n│   ├── anthropic.ts\n│   ├── novita.ts\n│   └── ollama.ts\n└── types.ts\n```\n\n### `lib/db/`\n\nDatabase configuration:\n\n```\nlib/db/\n├── index.ts               # Database client\n├── schema/                # Drizzle or Prisma schema\n│   ├── users.ts\n│   ├── conversations.ts\n│   └── documents.ts\n└── migrations/            # Migration files\n```\n\n### `lib/search/`\n\nVector search abstraction:\n\n```\nlib/search/\n├── index.ts               # Main exports\n├── config.ts\n├── providers/\n│   ├── qdrant.ts\n│   ├── weaviate.ts\n│   └── pgvector.ts\n└── types.ts\n```\n\n### `lib/auth/`\n\nAuthentication setup:\n\n```\nlib/auth/\n├── index.ts\n├── config.ts              # Auth provider config\n└── middleware.ts          # Auth middleware\n```\n\n## Configuration Files\n\n### `next.config.ts`\n\n```typescript\nimport type { NextConfig } from 'next';\n\nconst config: NextConfig = {\n  experimental: {\n    typedRoutes: true,\n  },\n};\n\nexport default config;\n```\n\n### `drizzle.config.ts`\n\n```typescript\nimport { defineConfig } from 'drizzle-kit';\n\nexport default defineConfig({\n  schema: './lib/db/schema',\n  out: './lib/db/migrations',\n  dialect: 'postgresql',\n  dbCredentials: {\n    url: process.env.DATABASE_URL!,\n  },\n});\n```\n\n## Development Scripts\n\n```json\n{\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"db:push\": \"drizzle-kit push\",\n    \"db:studio\": \"drizzle-kit studio\",\n    \"db:generate\": \"drizzle-kit generate\",\n    \"lint\": \"biome check .\",\n    \"format\": \"biome format --write .\"\n  }\n}\n```\n\n## Key Details\n\n- **Type-safe**: Full TypeScript across the stack\n- **Modular**: Each provider is a separate module\n- **Swappable**: Change providers via configuration\n- **Production-ready**: Includes error handling, logging","src/content/docs/project-structure.mdx","442637fd54a40049","project-structure.mdx","ais-config",{"id":120,"data":122,"body":125,"filePath":126,"digest":127,"legacyId":128,"deferredRender":20},{"title":123,"description":124},"AI Stack Config","Configuration file reference for AI Stack projects","## Overview\n\nAI Stack projects include a configuration file that tracks your stack choices and enables the CLI to enhance your project later.\n\n## Configuration File\n\nAfter creating a project, you'll find `ais.config.json` in your project root:\n\n```json\n{\n  \"$schema\": \"https://ai-stack.dev/schema.json\",\n  \"version\": \"1.0.0\",\n  \"createdAt\": \"2024-01-01T00:00:00Z\",\n  \"architecture\": \"nextjs\",\n  \"database\": \"neon\",\n  \"orm\": \"drizzle\",\n  \"auth\": \"better-auth\",\n  \"llm\": \"openai\",\n  \"vector\": \"qdrant\",\n  \"addons\": [\"tailwind\", \"biome\"]\n}\n```\n\n## Fields\n\n### Required Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `version` | string | CLI version used to create project |\n| `createdAt` | string | ISO 8601 timestamp |\n| `architecture` | string | Selected architecture |\n\n### Stack Options\n\n| Field | Type | Values |\n|-------|------|--------|\n| `database` | string | `neon`, `supabase`, `turso`, `sqlite` |\n| `orm` | string | `drizzle`, `prisma` |\n| `auth` | string | `better-auth`, `nextauth`, `clerk`, `none` |\n| `llm` | string | `openai`, `anthropic`, `novita`, `ollama` |\n| `vector` | string | `qdrant`, `weaviate`, `pgvector`, `pinecone`, `none` |\n| `addons` | array | `[\"tailwind\", \"biome\", \"pwa\", \"analytics\"]` |\n\n## Usage\n\n### Adding Features\n\nThe CLI uses this config to add compatible features:\n\n```bash\n# Add vector search to existing project\nnpx create-ai-stack-starter add vector\n\n# The CLI reads ais.config.json to ensure compatibility\n```\n\n### Validation\n\nThe config is validated against the schema:\n\n```bash\n# Validate your config\nnpx create-ai-stack-starter validate\n```\n\n## Schema\n\nThe full JSON schema is available at:\n\n```\nhttps://ai-stack.dev/schema.json\n```\n\nYou can reference it in your config for editor autocompletion:\n\n```json\n{\n  \"$schema\": \"https://ai-stack.dev/schema.json\"\n}\n```\n\n## Keep or Delete?\n\n- **Keep it** if you plan to use `npx create-ai-stack-starter add` to extend your project\n- **Safe to delete** if you won't use the CLI after initial creation\n\nThe config has no effect on your running application—it's only used by the CLI.\n\n## Example Configs\n\n### Minimal Next.js\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"architecture\": \"nextjs\",\n  \"database\": \"sqlite\",\n  \"auth\": \"none\",\n  \"llm\": \"ollama\"\n}\n```\n\n### Full Production Stack\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"architecture\": \"nextjs\",\n  \"database\": \"neon\",\n  \"orm\": \"drizzle\",\n  \"auth\": \"better-auth\",\n  \"llm\": \"openai\",\n  \"vector\": \"qdrant\",\n  \"addons\": [\"tailwind\", \"biome\", \"analytics\", \"docker\"]\n}\n```\n\n### FastAPI ML Team\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"architecture\": \"fastapi\",\n  \"database\": \"supabase\",\n  \"auth\": \"clerk\",\n  \"llm\": \"anthropic\",\n  \"vector\": \"weaviate\"\n}\n```","src/content/docs/ais-config.mdx","f6497818c3b52c29","ais-config.mdx","analytics",{"id":129,"data":131,"body":134,"filePath":135,"digest":136,"legacyId":137,"deferredRender":20},{"title":132,"description":133},"Analytics & Observability","Monitor your AI Stack application in production","## Overview\n\nAI Stack includes built-in support for tracking key metrics in your AI application: token usage, costs, latency, and error rates.\n\n## Quick Setup\n\nIf you selected the `analytics` addon during project creation, analytics is pre-configured. Otherwise, add it:\n\n```bash\nnpm install @ai-stack/analytics\n```\n\n## Token Usage Tracking\n\n### Automatic Tracking\n\nThe AI client automatically tracks token usage:\n\n```typescript\nimport { createCompletion } from '@/lib/ai';\n\nconst response = await createCompletion({\n  messages: [...],\n});\n\n// Access usage\nconsole.log(response.usage);\n// { promptTokens: 150, completionTokens: 50, totalTokens: 200 }\n```\n\n### Usage Events\n\n```typescript\nimport { analytics } from '@/lib/analytics';\n\nanalytics.on('completion', (event) => {\n  console.log({\n    provider: event.provider,\n    model: event.model,\n    tokens: event.usage.totalTokens,\n    cost: event.cost,\n    latency: event.latencyMs,\n  });\n});\n```\n\n## Cost Tracking\n\n### Automatic Cost Calculation\n\nCosts are calculated based on provider pricing:\n\n```typescript\nimport { getCostEstimate } from '@/lib/analytics';\n\nconst cost = getCostEstimate({\n  provider: 'openai',\n  model: 'gpt-4o',\n  promptTokens: 1000,\n  completionTokens: 500,\n});\n// $0.0175\n```\n\n### Cost Alerts\n\n```typescript\nimport { analytics } from '@/lib/analytics';\n\nanalytics.setCostAlert({\n  daily: 100,  // $100/day\n  monthly: 2000,\n  onAlert: (alert) => {\n    console.warn(`Cost alert: ${alert.type} limit reached`);\n  },\n});\n```\n\n## Latency Monitoring\n\n### Track Response Times\n\n```typescript\nimport { createCompletion } from '@/lib/ai';\n\nconst start = Date.now();\nconst response = await createCompletion({ ... });\nconst latency = Date.now() - start;\n\n// Built-in tracking\nconsole.log(response.meta.latencyMs);\n```\n\n### Latency Percentiles\n\n```typescript\nimport { analytics } from '@/lib/analytics';\n\nconst stats = await analytics.getLatencyStats({\n  period: '24h',\n});\n// { p50: 450, p95: 1200, p99: 2500 }\n```\n\n## Error Tracking\n\n### Error Events\n\n```typescript\nimport { analytics } from '@/lib/analytics';\n\nanalytics.on('error', (event) => {\n  console.error({\n    provider: event.provider,\n    error: event.error.message,\n    code: event.error.code,\n  });\n});\n```\n\n### Error Rate Monitoring\n\n```typescript\nconst errorStats = await analytics.getErrorStats({\n  period: '1h',\n});\n// { total: 10, rate: 0.02, byCode: { '429': 8, '500': 2 } }\n```\n\n## Dashboard Integration\n\n### PostHog\n\n```typescript\n// lib/analytics/posthog.ts\nimport posthog from 'posthog-js';\nimport { analytics } from '@/lib/analytics';\n\nanalytics.on('completion', (event) => {\n  posthog.capture('ai_completion', {\n    provider: event.provider,\n    model: event.model,\n    tokens: event.usage.totalTokens,\n    cost: event.cost,\n    latency: event.latencyMs,\n  });\n});\n```\n\n### Grafana\n\nExport metrics for Prometheus/Grafana:\n\n```typescript\nimport { collectMetrics } from '@/lib/analytics';\n\n// Prometheus-compatible metrics\napp.get('/metrics', (req, res) => {\n  res.set('Content-Type', 'text/plain');\n  res.send(collectMetrics());\n});\n```\n\n## Environment Variables\n\n```env\n# Analytics configuration\nANALYTICS_ENABLED=true\nANALYTICS_SAMPLE_RATE=1.0\n\n# PostHog\nNEXT_PUBLIC_POSTHOG_KEY=phc_...\nNEXT_PUBLIC_POSTHOG_HOST=https://app.posthog.com\n\n# Sentry (error tracking)\nSENTRY_DSN=https://...@sentry.io/...\n```\n\n## Best Practices\n\n1. **Sample high-volume requests** - Use sampling for production traffic\n2. **Aggregate metrics** - Don't store every request detail\n3. **Set cost alerts** - Avoid surprise bills\n4. **Monitor latency percentiles** - p95/p99 matter more than average\n5. **Track by user/org** - Identify heavy users\n\n## Data Retention\n\nConfigure how long to keep analytics data:\n\n```typescript\n// lib/analytics/config.ts\nexport const analyticsConfig = {\n  retention: {\n    raw: '7d',      // Raw events\n    hourly: '30d',  // Hourly aggregates\n    daily: '1y',    // Daily aggregates\n  },\n};\n```","src/content/docs/analytics.mdx","571bfe2b377d3486","analytics.mdx","contributing",{"id":138,"data":140,"body":143,"filePath":144,"digest":145,"legacyId":146,"deferredRender":20},{"title":141,"description":142},"Contributing","How to set up your environment and contribute changes","## Overview\n\nWe welcome contributions to AI Stack! This guide covers how to set up your development environment and contribute changes.\n\n> **Important:** Before starting work on major features, please open an issue first to discuss your proposal.\n\n## Repository Structure\n\n```\nai-stack/\n├── apps/\n│   └── web/               # Next.js frontend template\n├── packages/\n│   └── cli/               # CLI tool\n├── website/               # Documentation site\n└── docs/                  # Additional documentation\n```\n\n## Setup\n\n### Prerequisites\n\n- Node.js 20+ (LTS)\n- pnpm (recommended)\n- Git\n\n### Install\n\n```bash\ngit clone https://github.com/princepal9120/ai-stack.git\ncd ai-stack\npnpm install\n```\n\n## Development\n\n### CLI Development\n\n```bash\ncd packages/cli\n\n# Link for local testing\npnpm link --global\n\n# Run in watch mode\npnpm dev\n```\n\nTest the CLI anywhere:\n\n```bash\ncreate-ai-stack-starter my-test-app\n```\n\n### Website Development\n\n```bash\ncd website\npnpm dev\n```\n\nOpen http://localhost:4321 to see the documentation site.\n\n### Template Development\n\nTo modify the generated project templates:\n\n```bash\ncd apps/web\npnpm dev\n```\n\n## Contribution Flow\n\n1. **Open an issue** - Discuss your proposal\n2. **Fork the repository**\n3. **Create a feature branch**\n\n```bash\ngit checkout -b feat/my-feature\n```\n\n4. **Make changes** - Follow existing code style\n5. **Test your changes**\n\n```bash\n# Run tests\npnpm test\n\n# Type check\npnpm typecheck\n\n# Lint\npnpm lint\n```\n\n6. **Commit following conventional commits**\n\n```bash\ngit commit -m \"feat(cli): add new provider option\"\n```\n\n7. **Push and open a PR**\n\n```bash\ngit push origin feat/my-feature\n```\n\n## Commit Conventions\n\nUse conventional commits with appropriate scopes:\n\n| Type | Scope | Example |\n|------|-------|---------|\n| `feat` | `cli` | `feat(cli): add ollama provider` |\n| `fix` | `cli` | `fix(cli): resolve path issue on Windows` |\n| `feat` | `web` | `feat(web): add new landing section` |\n| `docs` | - | `docs: update installation guide` |\n| `chore` | - | `chore: update dependencies` |\n\n## Code Style\n\n- Use **TypeScript** everywhere\n- Format with **Biome**\n- Follow existing patterns in the codebase\n\n```bash\n# Format code\npnpm format\n\n# Check formatting\npnpm format:check\n```\n\n## Testing\n\n### Unit Tests\n\n```bash\npnpm test\n```\n\n### E2E Tests\n\n```bash\npnpm test:e2e\n```\n\n### Manual Testing\n\nCreate a test project with your local CLI:\n\n```bash\n# From packages/cli\npnpm link --global\n\n# In a test directory\ncreate-ai-stack-starter test-app --yes\ncd test-app\npnpm dev\n```\n\n## Documentation\n\nWhen adding features, update relevant documentation:\n\n1. **CLI options** - `website/src/content/docs/cli/options.mdx`\n2. **Guides** - `website/src/content/docs/guides/`\n3. **FAQ** - `website/src/content/docs/faq.mdx`\n\n## Getting Help\n\n- **GitHub Issues** - Bug reports and feature requests\n- **GitHub Discussions** - Questions and ideas\n- **Twitter** - [@princepal9120](https://twitter.com/princepal9120)\n\n## License\n\nBy contributing, you agree that your contributions will be licensed under the MIT License.","src/content/docs/contributing.mdx","f479029834422a44","contributing.mdx","faq",{"id":147,"data":149,"body":152,"filePath":153,"digest":154,"legacyId":155,"deferredRender":20},{"title":150,"description":151},"Frequently Asked Questions","Short answers to common questions","## General\n\n### What is AI Stack?\n\nAI Stack is an opinionated CLI that scaffolds production-ready AI applications with your choice of architecture, database, authentication, and AI providers. See the [Quick Start](/docs) to get started.\n\n### Do I need to install anything globally?\n\nNo. Run the CLI directly with your package manager:\n\n```bash\nnpx create-ai-stack-starter@latest\n```\n\n### Which package managers are supported?\n\nnpm, pnpm, bun, and yarn are all supported.\n\n### What Node.js version is required?\n\nNode.js 20+ (LTS recommended).\n\n### Can I use this with an existing project?\n\nThe CLI is designed for new projects. You can manually copy patterns from generated projects or wait for the `add` command to extend existing AI Stack projects.\n\n## Choosing Options\n\n### What architecture should I choose?\n\n- **Next.js Fullstack**: Best for rapid prototyping, Vercel deployment\n- **FastAPI + Next.js**: Best for ML teams, complex AI pipelines\n- **TypeScript Backend**: Best for TypeScript teams, edge deployment\n\n### Which database provider is best?\n\n- **Neon**: Serverless Postgres, great for Vercel\n- **Supabase**: Postgres with realtime, auth, storage\n- **Turso**: Edge SQLite, globally distributed\n- **SQLite**: Local development only\n\n### Drizzle or Prisma?\n\nBoth are excellent. Choose:\n- **Drizzle**: Lighter, SQL-first, better TypeScript inference\n- **Prisma**: More features, better tooling, larger ecosystem\n\n### Which LLM provider should I start with?\n\n- **OpenAI**: Proven, extensive documentation\n- **Anthropic**: Long context, great for documents\n- **Ollama**: Free, local development, privacy\n\n## Common Issues\n\n### API key not working\n\n1. Check the key is set in `.env.local`:\n\n```bash\ncat .env.local | grep API_KEY\n```\n\n2. Verify the key format is correct\n3. Check the key has required permissions\n\n### Database connection failing\n\n1. Check `DATABASE_URL` is set correctly\n2. Ensure the database is running\n3. Check network connectivity/firewalls\n\n### Vector database not connecting\n\n1. Check `QDRANT_URL` or equivalent is set\n2. Start the vector database container:\n\n```bash\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\n### Rate limits\n\nImplement retry logic:\n\n```typescript\ntry {\n  await createCompletion({ ... });\n} catch (error) {\n  if (error.status === 429) {\n    await sleep(1000);\n    // retry\n  }\n}\n```\n\n### Build errors\n\n1. Check all environment variables are set\n2. Run type check:\n\n```bash\npnpm typecheck\n```\n\n3. Clear cache and rebuild:\n\n```bash\nrm -rf .next node_modules\npnpm install\npnpm build\n```\n\n## Project Structure\n\n### Where are generated files?\n\nSee [Project Structure](/docs/project-structure) for detailed layouts by architecture.\n\n### Can I customize the generated code?\n\nYes! The generated code is yours. Modify anything to fit your needs.\n\n### How do I add new pages?\n\nFor Next.js, create files in `app/`:\n\n```\napp/\n├── about/\n│   └── page.tsx    # /about\n└── blog/\n    └── [slug]/\n        └── page.tsx # /blog/:slug\n```\n\n## Getting Help\n\n- **Documentation**: [Quick Start](/docs), [CLI Reference](/docs/cli)\n- **GitHub Issues**: Report bugs and request features\n- **GitHub Discussions**: Ask questions\n- **Twitter**: [@princepal9120](https://twitter.com/princepal9120)","src/content/docs/faq.mdx","941d213a6be7ab41","faq.mdx"]