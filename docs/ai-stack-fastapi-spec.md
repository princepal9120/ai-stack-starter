# AI Stack FastAPI Boilerplate - Complete Specification

**Production-grade FastAPI boilerplate for AI applications with RAG, vector databases, and zero vendor lock-in**

---

## üìã What You're Building

**AI Stack FastAPI** = Production-ready boilerplate specifically for FastAPI + AI

A CLI-based boilerplate generator that scaffolds complete AI applications with:
- ‚úÖ **FastAPI backend** (async Python, production-grade)
- ‚úÖ **Next.js frontend** (modern React UI with streaming)
- ‚úÖ **RAG pipelines** (retrieval-augmented generation ready)
- ‚úÖ **Vendor-agnostic** (swap vector DBs, LLMs via config)
- ‚úÖ **Built-in observability** (Langfuse, Prometheus)
- ‚úÖ **Zero lock-in** (OpenAI ‚Üí Anthropic in 1 env var change)
- ‚úÖ **Docker ready** (local dev to production)

---

## üöÄ Quick Start

```bash
# Install CLI
npm install -g ai-stack-fastapi

# Create project (FastAPI + Next.js)
ai-stack create my-rag-app

# Interactive setup (or use defaults)
# ‚úî Vector DB: Qdrant / Weaviate / Pgvector / Milvus?
# ‚úî LLM: OpenAI / Anthropic / Gemini / Ollama?
# ‚úî Auth: JWT / OAuth2 / Supabase?
# ‚úî Include observability? (Langfuse + Prometheus)

cd my-rag-app
docker-compose up -d    # Start Postgres, Redis, Qdrant
npm run dev            # Start FastAPI + Next.js

# Immediately functional:
# - Backend: http://localhost:8000
# - Frontend: http://localhost:3000
# - API docs: http://localhost:8000/docs
```

---

## üèó Monorepo Structure

```
my-ai-app/
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py               # FastAPI app entry
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py       # Chat endpoint (/v1/chat)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents.py  # Document upload (/v1/documents)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py # Embeddings endpoint
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth.py       # Auth routes (login, register, refresh)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deps.py           # Shared dependencies
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py       # Abstract LLMClient
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.py     # OpenAI client
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.py  # Anthropic client
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini.py     # Google Gemini client
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama.py     # Ollama (local models)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py   # Main RAG class
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py  # Vector search
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reranker.py   # Optional reranking
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt.py     # Prompt templates
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py       # Agent base class
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools.py      # Tool definitions
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ executor.py   # Tool execution
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chains/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ streaming.py  # Streaming chains
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Abstract VectorStore
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qdrant.py         # Qdrant adapter
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weaviate.py       # Weaviate adapter
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pgvector.py       # PostgreSQL pgvector
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ milvus.py         # Milvus adapter
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py           # User model
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.py       # Document model
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation.py   # Chat history
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.py      # Embedding metadata
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py        # Pydantic request/response schemas
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_service.py   # User business logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_service.py   # Chat orchestration
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_repo.py      # Database access
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_repo.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_repo.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py           # Base repository
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Settings (pydantic-settings)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py       # JWT, password hashing
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py       # SQLAlchemy setup
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vectordb.py       # Vector store initialization
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache.py          # Redis setup
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observability.py  # Langfuse, OpenTelemetry
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py        # Structured logging
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py     # Custom exceptions
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ celery_app.py     # Celery configuration
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_tasks.py # Async document processing
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_tasks.py # Embedding generation
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cleanup_tasks.py  # Scheduled tasks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py           # Auth middleware
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py        # Request logging
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rate_limit.py     # Rate limiting
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ error_handler.py  # Global error handling
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ tokenizer.py      # Token counting
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ validators.py     # Input validation
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ text_splitter.py  # Document chunking
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ helpers.py        # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py           # Pytest fixtures
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_chat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_documents.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_ai/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_rag_pipeline.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_llm_clients.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_vector_search.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_chat_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fixtures/
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ sample_docs.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ mock_llm.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ init_db.py            # Initialize database
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ seed_data.py          # Seed sample data
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_user.py        # CLI script to create user
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index_documents.py    # Index documents into vector DB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements-dev.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .dockerignore
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytest.ini
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .env.example
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ frontend/
‚îÇ       ‚îú‚îÄ‚îÄ app/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx            # Root layout
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx              # Home page
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx          # Chat interface
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useChat.ts        # Chat hook
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ChatInterface.tsx # Chat component
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ documents/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx          # Document management
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ UploadForm.tsx    # Upload component
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth/
‚îÇ       ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ [...nextauth].ts  # NextAuth config (optional)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ (auth)/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ login/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ signup/
‚îÇ       ‚îú‚îÄ‚îÄ components/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ChatMessage.tsx
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ MessageInput.tsx
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ DocumentUpload.tsx
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Navbar.tsx
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Loading.tsx
‚îÇ       ‚îú‚îÄ‚îÄ hooks/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ useChat.ts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ useAuth.ts
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ useApi.ts
‚îÇ       ‚îú‚îÄ‚îÄ lib/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api.ts                # API client
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ auth.ts               # Auth utilities
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ types.ts              # TypeScript types
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ constants.ts          # Constants
‚îÇ       ‚îú‚îÄ‚îÄ styles/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ globals.css
‚îÇ       ‚îú‚îÄ‚îÄ public/
‚îÇ       ‚îú‚îÄ‚îÄ .env.example
‚îÇ       ‚îú‚îÄ‚îÄ package.json
‚îÇ       ‚îú‚îÄ‚îÄ next.config.js
‚îÇ       ‚îú‚îÄ‚îÄ tailwind.config.js
‚îÇ       ‚îî‚îÄ‚îÄ tsconfig.json
‚îÇ
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îú‚îÄ‚îÄ ai-core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Abstract LLMClient
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py        # LLM client factory
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py       # Main RAG class
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py      # Search logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reranker.py       # Optional reranking
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt.py         # Prompt templates
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stream_handler.py # SSE/WebSocket streaming
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_iterator.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ py.typed
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ vector-db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py               # Abstract VectorStore
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapters/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qdrant.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weaviate.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pgvector.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ milvus.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py            # Vector store factory
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ auth/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jwt_auth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ oauth.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ password.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ shared-types/
‚îÇ       ‚îú‚îÄ‚îÄ src/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ py.typed
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api.py                # Request/response types
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ models.py             # Domain models
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ config.py             # Config types
‚îÇ       ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml            # Local dev (Postgres, Redis, Qdrant)
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.prod.yml       # Production setup
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                    # FastAPI app Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.celery             # Celery worker Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ k8s/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml           # FastAPI deployment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ secret.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hpa.yaml                  # Horizontal Pod Autoscaler
‚îÇ   ‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ provider.tf
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îú‚îÄ‚îÄ deploy.sh
‚îÇ       ‚îú‚îÄ‚îÄ migrate.sh
‚îÇ       ‚îî‚îÄ‚îÄ health_check.sh
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ README.md                     # Project overview
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md                 # 5-minute getting started
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md               # System design & data flow
‚îÇ   ‚îú‚îÄ‚îÄ SETUP.md                      # Local development guide
‚îÇ   ‚îú‚îÄ‚îÄ API.md                        # API endpoints reference
‚îÇ   ‚îú‚îÄ‚îÄ RAG_GUIDE.md                  # RAG pipeline tutorial
‚îÇ   ‚îú‚îÄ‚îÄ LLM_PROVIDERS.md              # How to switch LLM providers
‚îÇ   ‚îú‚îÄ‚îÄ VECTOR_DB.md                  # Vector store configuration
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md                 # Production deployment
‚îÇ   ‚îú‚îÄ‚îÄ DATABASE.md                   # Database schema & migrations
‚îÇ   ‚îú‚îÄ‚îÄ TESTING.md                    # Testing strategy & examples
‚îÇ   ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md            # Common issues & solutions
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ test.yml                  # Run pytest on PR
‚îÇ       ‚îú‚îÄ‚îÄ lint.yml                  # Linting checks
‚îÇ       ‚îú‚îÄ‚îÄ build.yml                 # Build & push Docker images
‚îÇ       ‚îú‚îÄ‚îÄ deploy-staging.yml        # Deploy to staging
‚îÇ       ‚îî‚îÄ‚îÄ deploy-prod.yml           # Deploy to production
‚îÇ
‚îú‚îÄ‚îÄ turbo.json                        # Turborepo config (optional)
‚îú‚îÄ‚îÄ pyproject.toml                    # Root Python config
‚îú‚îÄ‚îÄ package.json                      # Root NPM config
‚îú‚îÄ‚îÄ docker-compose.yml                # Local dev environment
‚îú‚îÄ‚îÄ .env.example                      # Template for environment variables
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .pre-commit-config.yaml          # Pre-commit hooks (black, isort, ruff)
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ Makefile                          # Convenient commands
‚îú‚îÄ‚îÄ CONTRIBUTING.md                   # Contribution guidelines
‚îî‚îÄ‚îÄ README.md                         # Project root README
```

---

## üõ† Tech Stack (FastAPI Specific)

### Backend Stack
- **Framework:** FastAPI 0.104+ (async Python web framework)
- **ASGI Server:** Uvicorn (production-grade)
- **ORM:** SQLAlchemy 2.0 + Alembic (database & migrations)
- **Database:** PostgreSQL 15+ (with optional pgvector extension)
- **Validation:** Pydantic V2 (type hints & runtime validation)
- **Cache:** Redis 7+ (sessions, caching, job queue)
- **Task Queue:** Celery + Redis (async background jobs)
- **Testing:** pytest + pytest-asyncio + testcontainers
- **Linting:** ruff, black, isort, mypy
- **API Docs:** Auto-generated OpenAPI (Swagger UI, ReDoc)

### AI/ML Stack
- **LLM Clients:** 
  - OpenAI SDK (GPT-4, GPT-3.5-turbo)
  - Anthropic SDK (Claude)
  - Google Generative AI (Gemini)
  - Ollama (local models)
- **Embeddings:** Built-in from LLM providers
- **RAG:** LangChain or custom minimal implementation
- **Vector Databases (swappable):**
  - Qdrant (recommended, best for production)
  - Weaviate (hybrid search features)
  - pgvector (PostgreSQL extension, simple)
  - Milvus (distributed, scalable)
- **Reranking:** Optional cross-encoder (sentence-transformers)

### Frontend Stack
- **Framework:** Next.js 16+ (React 19+)
- **Styling:** Tailwind CSS 4+
- **Components:** shadcn/ui (unstyled, accessible components)
- **State Management:** React hooks + Context API (or SWR for data fetching)
- **API Client:** fetch / axios
- **Forms:** react-hook-form + zod validation
- **UI Components:** Radix UI primitives
- **Type Safety:** TypeScript 5+

### Observability & Monitoring
- **LLM Tracing:** Langfuse (prompt logging, cost tracking)
- **Metrics:** Prometheus + Grafana (CPU, memory, latency, API calls)
- **Distributed Tracing:** OpenTelemetry (optional)
- **Error Tracking:** Sentry (optional)
- **Logging:** Python logging + JSON formatter

### DevOps & Deployment
- **Containerization:** Docker + Docker Compose
- **Orchestration:** Kubernetes (optional, manifests provided)
- **CI/CD:** GitHub Actions (automated tests, builds, deployment)
- **Infrastructure as Code:** Terraform (optional)
- **Hosting Options:**
  - Fly.io (simple deployment)
  - Railway (developer-friendly)
  - Render (serverless)
  - AWS ECS/EKS (enterprise)
  - Self-hosted Docker Swarm

---

## üéØ Core Abstractions (Vendor-Agnostic)

### 1. LLM Client Interface

```python
# packages/ai-core/llm/base.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import AsyncIterator

@dataclass
class LLMResponse:
    text: str
    tokens_used: int
    model: str
    cost: float = 0.0

class LLMClient(ABC):
    @abstractmethod
    async def complete(
        self, 
        prompt: str, 
        max_tokens: int = 2000,
        temperature: float = 0.7
    ) -> LLMResponse:
        """Generate completion from prompt"""
        pass
    
    @abstractmethod
    async def stream(
        self, 
        prompt: str
    ) -> AsyncIterator[str]:
        """Stream response token-by-token"""
        pass
    
    @abstractmethod
    async def embed(self, text: str) -> list[float]:
        """Generate embedding vector"""
        pass
```

**Usage in FastAPI:**

```python
# apps/backend/app/core/config.py
from packages.ai_core.llm import OpenAIClient, AnthropicClient, OllamaClient

# Select based on env var
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai")

llm_clients = {
    "openai": OpenAIClient(api_key=settings.OPENAI_API_KEY),
    "anthropic": AnthropicClient(api_key=settings.ANTHROPIC_API_KEY),
    "ollama": OllamaClient(base_url="http://localhost:11434"),
}

llm = llm_clients[LLM_PROVIDER]
```

**Swap LLM (NO CODE CHANGES):**
```bash
# Change 1 env var
LLM_PROVIDER=openai   # Current
LLM_PROVIDER=anthropic  # Switch to Claude (restart FastAPI)

# FastAPI automatically uses correct client
```

### 2. Vector Store Interface

```python
# packages/vector-db/base.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional

@dataclass
class SearchResult:
    id: str
    text: str
    metadata: dict
    score: float

class VectorStore(ABC):
    @abstractmethod
    async def upsert(
        self, 
        vectors: list[list[float]], 
        metadata: list[dict], 
        ids: list[str]
    ) -> None:
        """Add or update vectors in store"""
        pass
    
    @abstractmethod
    async def search(
        self, 
        query_vector: list[float], 
        top_k: int = 10,
        filters: Optional[dict] = None
    ) -> list[SearchResult]:
        """Search for similar vectors"""
        pass
    
    @abstractmethod
    async def delete(self, ids: list[str]) -> None:
        """Delete vectors by ID"""
        pass
```

**Usage in FastAPI:**

```python
# apps/backend/app/core/vectordb.py
from packages.vector_db import QdrantStore, WeaviateStore, PgVectorStore

vector_db_clients = {
    "qdrant": QdrantStore(url="http://localhost:6333"),
    "weaviate": WeaviateStore(url="http://localhost:8080"),
    "pgvector": PgVectorStore(db_connection=db),
}

vector_store = vector_db_clients[os.getenv("VECTOR_DB_PROVIDER", "qdrant")]
```

**Swap Vector DB (10 minutes, NO CODE REWRITE):**
```bash
# 1. Update .env
VECTOR_DB_PROVIDER=qdrant    # Current
VECTOR_DB_PROVIDER=weaviate  # Swap to Weaviate

# 2. Update docker-compose.yml (remove Qdrant, add Weaviate)

# 3. Restart FastAPI
docker-compose restart backend

# Done! Everything else stays the same
```

### 3. RAG Pipeline (Production-Ready)

```python
# packages/ai-core/rag/pipeline.py
class RAGPipeline:
    def __init__(
        self,
        vector_store: VectorStore,
        llm_client: LLMClient,
        reranker: Optional[Reranker] = None,
        max_context_tokens: int = 2000,
    ):
        self.vector_store = vector_store
        self.llm_client = llm_client
        self.reranker = reranker
        self.max_context_tokens = max_context_tokens
    
    async def query(self, question: str, user_id: str) -> dict:
        """
        Full RAG pipeline:
        1. Embed question
        2. Search vector store
        3. Rerank results
        4. Build context
        5. Generate answer
        6. Track cost & latency
        """
        import time
        start_time = time.time()
        
        # Step 1: Embed question
        query_embedding = await self.llm_client.embed(question)
        
        # Step 2: Search vector store
        search_results = await self.vector_store.search(
            query_embedding, 
            top_k=10
        )
        
        # Step 3: Rerank (optional)
        if self.reranker:
            search_results = await self.reranker.rerank(
                question, 
                search_results
            )
        
        # Step 4: Build context
        context = self._build_context(search_results)
        
        # Step 5: Generate answer
        prompt = self._build_prompt(question, context)
        response = await self.llm_client.complete(prompt)
        
        # Step 6: Track metrics
        latency_ms = (time.time() - start_time) * 1000
        
        return {
            "answer": response.text,
            "sources": [r.id for r in search_results],
            "tokens_used": response.tokens_used,
            "cost": response.cost,
            "latency_ms": latency_ms,
        }
    
    def _build_context(self, results: list[SearchResult]) -> str:
        """Join search results into context"""
        parts = []
        token_count = 0
        
        for result in results:
            text = f"[{result.id}]\n{result.text}"
            tokens = len(text.split())
            
            if token_count + tokens > self.max_context_tokens:
                break
            
            parts.append(text)
            token_count += tokens
        
        return "\n\n---\n\n".join(parts)
    
    def _build_prompt(self, question: str, context: str) -> str:
        """Format prompt with context"""
        return f"""You are a helpful AI assistant. Use the provided context to answer the user's question accurately.

Context:
{context}

User Question: {question}

Answer:"""
```

**Usage in FastAPI endpoint:**

```python
# apps/backend/app/api/v1/chat.py
from fastapi import APIRouter, Depends
from packages.ai_core.rag.pipeline import RAGPipeline

router = APIRouter(prefix="/v1", tags=["chat"])

@router.post("/chat")
async def chat(
    message: str,
    user = Depends(get_current_user),
    rag: RAGPipeline = Depends(get_rag_pipeline)
):
    """Chat endpoint with RAG"""
    result = await rag.query(message, user_id=user.id)
    
    # Log to Langfuse
    langfuse.trace(
        name="rag_chat",
        input={"message": message},
        output=result,
        user_id=user.id,
    )
    
    return result
```

---

## üì¶ CLI Commands

### `ai-stack create`

```bash
ai-stack create my-rag-app

‚úî Vector Database
  ‚óØ Qdrant (self-hosted, recommended)
  ‚óØ Weaviate (hybrid search)
  ‚óØ PostgreSQL pgvector (simple)
  ‚óØ Milvus (distributed, scalable)

‚úî LLM Provider
  ‚óØ OpenAI (GPT-4, default)
  ‚óØ Anthropic (Claude, longer context)
  ‚óØ Google Gemini (multimodal)
  ‚óØ Ollama (local, open-source)

‚úî Authentication
  ‚óØ JWT (stateless, recommended)
  ‚óØ OAuth2 + PKCE (social login)

‚úî Additional Features
  ‚óØ Observability (Langfuse + Prometheus)
  ‚óØ Background jobs (Celery + Redis)
  ‚óØ Kubernetes manifests
  ‚óØ GitHub Actions CI/CD

‚ú® Creating project...
üê≥ Setting up Docker environment...
üì¶ Installing dependencies...
‚úÖ Done!

Next steps:
  cd my-rag-app
  docker-compose up
  python apps/backend/scripts/init_db.py
  make dev
```

### `ai-stack swap`

```bash
# Swap vector database
ai-stack swap vectordb --to weaviate

# Swap LLM provider
ai-stack swap llm --to anthropic

# Swap auth
ai-stack swap auth --to oauth2

# Swap multiple with preset
ai-stack swap all --preset=gcp-vertex-ai
```

### `ai-stack add`

```bash
ai-stack add langfuse       # Add LLM observability
ai-stack add reranking      # Add cross-encoder reranking
ai-stack add tools          # Add function calling for agents
ai-stack add websockets     # Add WebSocket streaming
ai-stack add kubernetes     # Add K8s manifests
ai-stack add monitoring     # Add Prometheus + Grafana
```

### `ai-stack dev`

```bash
ai-stack dev

# Starts:
# - Docker containers (Postgres, Redis, Qdrant)
# - FastAPI backend (reload on changes)
# - Next.js frontend (with hot reload)
# - Celery worker (for async jobs)

# Output:
‚ú® Ready to code!
  Backend:  http://localhost:8000
  Frontend: http://localhost:3000
  API Docs: http://localhost:8000/docs
```

---

## üöÄ Example: RAG Chatbot (Complete)

After `ai-stack create my-chatbot`:

**Backend endpoint:**
```python
# apps/backend/app/api/v1/chat.py
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

router = APIRouter(prefix="/v1", tags=["chat"])

class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    sources: list[str]
    tokens_used: int

@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    user = Depends(get_current_user),
    rag: RAGPipeline = Depends(get_rag_pipeline),
    db: Session = Depends(get_db),
):
    """Chat with RAG retrieval"""
    try:
        result = await rag.query(request.message, user.id)
        
        # Save to conversation history
        conversation = await chat_service.save_conversation(
            db=db,
            user_id=user.id,
            conversation_id=request.conversation_id,
            message=request.message,
            response=result["answer"],
            sources=result["sources"],
        )
        
        return ChatResponse(
            response=result["answer"],
            sources=result["sources"],
            tokens_used=result["tokens_used"],
        )
    
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail="Chat failed")

@router.post("/chat/stream")
async def chat_stream(
    request: ChatRequest,
    user = Depends(get_current_user),
    rag: RAGPipeline = Depends(get_rag_pipeline),
):
    """Streaming chat endpoint"""
    async def generate():
        async for token in rag.llm_client.stream(request.message):
            yield f"data: {json.dumps({'token': token})}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

**Frontend component:**
```typescript
// apps/frontend/app/chat/page.tsx
"use client";
import { useState, useRef, useEffect } from "react";

export default function ChatPage() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [loading, setLoading] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const sendMessage = async () => {
    if (!input.trim()) return;

    setLoading(true);
    setMessages((prev) => [...prev, { role: "user", content: input }]);

    try {
      const res = await fetch("/api/v1/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ message: input }),
      });

      const data = await res.json();

      setMessages((prev) => [
        ...prev,
        {
          role: "assistant",
          content: data.response,
          sources: data.sources,
        },
      ]);
    } catch (error) {
      console.error("Error:", error);
    } finally {
      setLoading(false);
      setInput("");
    }
  };

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  return (
    <div className="flex flex-col h-screen">
      {/* Chat messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.map((msg, i) => (
          <div
            key={i}
            className={`flex ${
              msg.role === "user" ? "justify-end" : "justify-start"
            }`}
          >
            <div
              className={`max-w-md p-3 rounded-lg ${
                msg.role === "user"
                  ? "bg-blue-500 text-white"
                  : "bg-gray-200 text-black"
              }`}
            >
              <p>{msg.content}</p>
              {msg.sources && (
                <div className="mt-2 text-sm opacity-75">
                  Sources: {msg.sources.join(", ")}
                </div>
              )}
            </div>
          </div>
        ))}
        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <div className="border-t p-4 flex gap-2">
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyDown={(e) => e.key === "Enter" && sendMessage()}
          placeholder="Ask something..."
          className="flex-1 border rounded px-3 py-2"
          disabled={loading}
        />
        <button
          onClick={sendMessage}
          disabled={loading}
          className="bg-blue-500 text-white px-4 py-2 rounded disabled:opacity-50"
        >
          {loading ? "..." : "Send"}
        </button>
      </div>
    </div>
  );
}
```

---

## ‚úÖ What You Get (Day 1)

```
‚úÖ FastAPI Backend with:
   - 5 REST endpoints (chat, documents, embeddings, auth, health)
   - JWT authentication with refresh tokens
   - Database models (users, documents, conversations)
   - Async/await throughout
   - RAG pipeline ready to use
   - Celery background job support
   - Structured JSON logging
   - Prometheus metrics export

‚úÖ Next.js Frontend with:
   - Chat interface with conversation history
   - Document upload component
   - Authentication pages (login/signup)
   - Responsive Tailwind CSS design
   - TypeScript type safety
   - API client library

‚úÖ Production Infrastructure:
   - Docker Compose (Postgres, Redis, Qdrant)
   - Dockerfile for FastAPI (multi-stage)
   - Docker Dockerfile for Celery worker
   - Health checks & readiness probes
   - Database migrations (Alembic)
   - Environment variable management

‚úÖ Testing & Quality:
   - Unit tests (services, utilities, repositories)
   - Integration tests (database, vector store)
   - API endpoint tests
   - Fixtures & mocking
   - pytest configuration
   - 80%+ code coverage

‚úÖ CI/CD & DevOps:
   - GitHub Actions (test, lint, build)
   - Docker image builds & push
   - Automated deployment options
   - Security scanning
   - Code quality checks

‚úÖ Documentation:
   - Architecture diagram
   - API reference (auto-generated OpenAPI)
   - RAG pipeline guide
   - LLM provider switching guide
   - Deployment instructions
   - Troubleshooting guide

‚úÖ Observability:
   - Langfuse integration (hooks installed)
   - Prometheus metrics (CPU, memory, latency)
   - Structured logging
   - Error tracking hooks
   - Performance monitoring
```

---

## üîÑ Swap Workflows

### Change LLM: OpenAI ‚Üí Anthropic

```bash
# 1. Update .env
OPENAI_API_KEY=sk_...      # Remove
ANTHROPIC_API_KEY=sk_...   # Add

LLM_PROVIDER=openai        # Change to:
LLM_PROVIDER=anthropic

# 2. Restart FastAPI
docker-compose restart backend

# 3. Done! No code changes, no redeployment of frontend
# All API contracts stay the same
```

### Change Vector DB: Qdrant ‚Üí Weaviate

```bash
# 1. Update docker-compose.yml
# Remove qdrant service, add weaviate service

# 2. Update .env
VECTOR_DB_PROVIDER=qdrant    # Change to:
VECTOR_DB_PROVIDER=weaviate

QDRANT_URL=http://localhost:6333  # Remove
WEAVIATE_URL=http://localhost:8080  # Add

# 3. Re-index documents
docker-compose up -d weaviate
python apps/backend/scripts/index_documents.py

# 4. Done! FastAPI code unchanged
```

---

## üìã Installation

```bash
# Global install
npm install -g ai-stack-fastapi

# Create project
ai-stack create my-ai-app

# Or clone and build
git clone https://github.com/ai-stack-team/ai-stack-fastapi
cd ai-stack-fastapi
npm run build
npm run dev -- create test-app
```

---

## üö¢ Deployment Options

### Fly.io (5 minutes)
```bash
flyctl launch
flyctl deploy
```

### Railway (3 minutes)
```bash
railway link
railway up
```

### Docker to any host
```bash
docker build -t my-ai-app .
docker run -p 8000:8000 my-ai-app
```

---

## üìä Performance Characteristics

- **First message latency:** 200-500ms (depends on LLM)
- **Streaming response time:** <100ms first token
- **Vector search:** <50ms for 100k vectors
- **Throughput:** 100+ concurrent users per instance
- **Cost per query:** ~$0.001-0.1 (depends on LLM)

---

## ‚ú® What Makes It Special

1. **Zero Vendor Lock-In** - Swap LLMs, vector DBs via 1 env var change
2. **RAG Out-of-Box** - Production-ready retrieval pipeline
3. **Full Type Safety** - Python + TypeScript throughout
4. **Async-First** - Non-blocking I/O, high concurrency
5. **Observable** - Built-in Langfuse, Prometheus hooks
6. **Production-Ready** - Security, testing, CI/CD configured
7. **Easy to Extend** - Clear abstractions for custom logic

---

## üéØ Roadmap

**Week 1-2:** MVP
- [ ] FastAPI boilerplate
- [ ] Qdrant + OpenAI integration
- [ ] Docker Compose setup
- [ ] Publish CLI to npm

**Week 3-4:** Features
- [ ] Multiple vector DB adapters
- [ ] Multiple LLM providers
- [ ] Swap commands
- [ ] Add Langfuse observability

**Week 5-6:** Polish
- [ ] Example projects
- [ ] Video tutorials
- [ ] Kubernetes manifests
- [ ] Production deployment guide

---

**Everything you need to build production AI apps, today.** üöÄ

